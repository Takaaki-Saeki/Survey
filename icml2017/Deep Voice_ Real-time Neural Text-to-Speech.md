# Deep Voice: Real-time Neural Text-to-Speech
###### tags: `survey`

## 1. 概要
DNNによるリアルタイムなend-to-end音声合成システムDeep Voiceを提案した論文．Deep Voiceは大まかに5つのモデルからなる．
1. 音素の境界を定める分割モデル
2. 書記素から音素に変換するモデル
3. 音素の持続時間を予測するモデル
4. 基本周波数を予測するモデル
5. 波形合成を行うモデル

分割モデルとしては，connectionist temporal classification lossを用いたDNNによる音素境界の予測手法を提案している．波形合成モデルとしては，パラメータ数を減らして，元の論文のものより高速な訓練が可能なWaveNetを用いている．各々の部分にDNNを用いることにより，本手法は，人手でのチューニングや関連領域の専門知識が必要であった従来のTTS手法よりも単純で柔軟なものになったとしている．
論文の最後には，Deep Voiceによる推論がリアルタイムで動作し，最適化されCPU&GPUを用いたWaveNetによる推論が既存の自走よりも最大400倍速くなることを示す．

## 2. 先行研究と比較して何がすごいのか
既存研究では，従来のTTSシステムを構成する部分の代わりを果たすようなシステムが多く提案されてきたが，それらのシステムではTTS全体を解くことはできず，特殊な人手の特徴量を使っている．

Deep Voiceの優位性は以下のようになる．
第一に，Deep Voiceでは，単体でTTSの問題を解くことができ，pretrainingされた別のTTSモデルは必要ない．音声データと，それに対応するテキストデータがあれば0から学習を行うことができる．
第二に，Deep Voiceは人手の特徴量を最小限に減らすことが可能である．使うのは，書記素-音素変換のためのhot encodingされた文字，one hot encodingされた音素とアクセント，ミリセカンドオーダーの音素の持続時間，f0予測モデルによって計算される，正規化された対数基本周波数だけである．例えば，従来のWaveNetでは，音節の数や音節の位置，音素内のフレームの位置，スペクトルの動的特徴量などの特徴量を用いており，Cahr2wavでは，WORLDによって得られたボコーダ特徴量に依存している．
最後に，Deep Voiceでは，リアルタイムで推論することが可能な，実用化に向けたシステムを構築している．Deep Voiceではほんの数秒で波形を合成することができるが，例えばWaveNetでは，一秒の波形を合成するのに数分を要してしまう．

## 3. 手法のキモ
Deep VoiceによるTTSシステムは，概要欄で述べたような5つのシステムからなる．
推論の過程では，テキストが書記素から音素へ変換するモデル，または音素辞書に入り，音素が生成される．さらに，この音素は音素の持続時間のモデルとf0予測のモデルの入力として与えられ，f０系列を生成する．最後に，音素，音素の持続時間，f0の3つの特徴量が，最終的な波形を得るための条件付けを行う入力特徴量として与えられる．
他のモデルと異なり，分割モデルは推論時には用いられず，訓練データの音素境界を作るのに使われる．音素境界とはすなわち音素の持続時間を表し，音素の持続モデルを学習するのに用いることが可能である．また，音素と音素境界，f0を持つ音声データによって波形合成モデルの訓練を行うことができる．
学習と推論の概念図は以下の通り．
![](https://i.imgur.com/83kkiWl.png)

各モデルについて，非常に簡易的な説明を述べる．
- grapheme-to-phoneme model
(Yao & Zweig, 2015)によるencoder-decoderモデルが基礎となっているが，ここでは，encoderとしてGRUによるmulti-layer bidirectional encoderを用い，decoderとして同等に深い層をもつGRU decoderを用いている．
- segmentation model
音声認識の知見から，テキストと音声の対応づけに向けたconectionist temporal classification(CTC) loss，(Amodei et al.,, 2015)のSOTAなconvolutional rnnのアーキテクチャを採用した．また，単体の音素の予測を行うだけでは，音声データと音素の大体のアラインメントを撮るには十分だが，正確な音素境界を検出するには不十分であるため，音素のペアを予測するようにした．
- phoneme duration and f0 model
一つのアーキテクチャで音素の継続時間の予測と時間依存のf0の予測を同時に行うことができる．モデルの入力はアクセント付きの音素だが，これらはそれぞれone hotベクトルになっている．モデルのアーキテクチャは，256ユニットをもつ2そうの全結合層からなっており，それぞれ128のGRUセルをもつ二つのundirectional reccurent layersと，全結合の出力層からなっている．
- audio synthesis model
WaveNetのアーキテクチャ，パラメータの取り方を変えることにより，パフォーマンスが高く，学習時間が早いモデルを実現している．

## 4. 実験・検証方法
各モデルごとに実験的評価を行っている．
また，CPU・GPUそれぞれについて推論の高速化・最適化を行い，遅延を評価している．

## 5. 議論
細かい数字が多いのでとりあえず割愛．
ある条件下でGPUで推論を行なった場合，遅延が0.17sにまで抑えられるとしている．

## 6. 次に読むべき論文
- Rao et al., "GRAPHEME-TO-PHONEME CONVERSION USING LONG SHORT-TERM MEMORY RECURRENT NEURAL NETWORKS"